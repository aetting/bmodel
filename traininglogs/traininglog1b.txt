Training set: trainingpairs-br-origfulleng
Embeddings: brouwerGloVe-100.txt
Binary: True
Context 200, Retrieval 80
Vocab 34, Embedding 100

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 196.405452609  lr: 0.2
loss: 104.211913247  lr: 0.2
loss: 78.8840304734  lr: 0.2
loss: 83.3136551315  lr: 0.2
loss: 75.1225112122  lr: 0.2
loss: 70.0713968002  lr: 0.2
loss: 62.529210775  lr: 0.2
loss: 67.8856003263  lr: 0.2
loss: 60.3510867732  lr: 0.2
loss: 63.217765977  lr: 0.2
loss: 58.5282867082  lr: 0.2
loss: 67.1659453278  lr: 0.2
loss: 65.2455415813  lr: 0.2
loss: 65.0314704091  lr: 0.2
loss: 61.6360757296  lr: 0.2
loss: 54.7211580117  lr: 0.2
loss: 59.3372428675  lr: 0.2
loss: 57.9471101753  lr: 0.2
loss: 64.8206001408  lr: 0.2
loss: 63.387745079  lr: 0.2
loss: 62.3181453024  lr: 0.2
loss: 57.5899125476  lr: 0.2
loss: 55.65859603  lr: 0.2
loss: 53.3219195974  lr: 0.2
loss: 60.2114682799  lr: 0.2
loss: 60.725084429  lr: 0.2
loss: 50.4537540541  lr: 0.2
loss: 60.5257694864  lr: 0.2
loss: 62.7475677361  lr: 0.2
loss: 59.3262016153  lr: 0.2
loss: 59.580258472  lr: 0.2
loss: 62.6444579639  lr: 0.2
loss: 58.3651328366  lr: 0.2
loss: 60.8193145552  lr: 0.2
loss: 62.3018418711  lr: 0.2
loss: 56.0815072298  lr: 0.2
loss: 57.9757372912  lr: 0.2
loss: 62.8478289935  lr: 0.2
loss: 60.388848349  lr: 0.2
loss: 57.8312591981  lr: 0.2
loss: 60.3507192976  lr: 0.2
loss: 55.332827395  lr: 0.2
loss: 63.1789420951  lr: 0.2
loss: 57.7486136681  lr: 0.2
loss: 63.3638998217  lr: 0.2
loss: 60.9246445816  lr: 0.2
loss: 50.959827473  lr: 0.2
loss: 59.3790431048  lr: 0.2
loss: 56.3153122304  lr: 0.2
loss: 58.159611227  lr: 0.2
loss: 61.2806292002  lr: 0.2
loss: 56.8756176855  lr: 0.2
loss: 53.1737502373  lr: 0.2
loss: 64.351196194  lr: 0.2
loss: 55.7870461472  lr: 0.2
loss: 62.4412968562  lr: 0.2
loss: 58.1856314336  lr: 0.2
loss: 55.661007795  lr: 0.2
loss: 52.9503929418  lr: 0.2
loss: 58.0605572758  lr: 0.2
loss: 58.3735060213  lr: 0.2
loss: 55.986096408  lr: 0.2
loss: 58.6934611162  lr: 0.2
loss: 60.0200645168  lr: 0.2
loss: 57.9653172765  lr: 0.2
loss: 57.80974306  lr: 0.2
loss: 60.4093594846  lr: 0.2
loss: 56.8579227047  lr: 0.2
loss: 57.3173838689  lr: 0.2
loss: 53.7121212875  lr: 0.2

Correct: 16000 out of 16000 (1.0)

TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (234 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 174.458972991  lr: 0.2
loss: 134.030373245  lr: 0.2
loss: 102.749009543  lr: 0.2
loss: 92.2055399669  lr: 0.2
loss: 89.7272895977  lr: 0.2
loss: 93.5876034962  lr: 0.2
loss: 69.5127810179  lr: 0.2
loss: 69.9690735738  lr: 0.2
loss: 59.2555803271  lr: 0.2
loss: 54.602579451  lr: 0.2
loss: 58.6180541603  lr: 0.2
loss: 54.7989791325  lr: 0.2
loss: 63.7404559316  lr: 0.2
loss: 55.6239936914  lr: 0.2
loss: 62.6554347956  lr: 0.2
loss: 56.4767245358  lr: 0.2
loss: 61.2004398554  lr: 0.2
loss: 56.9296964995  lr: 0.2
loss: 59.1542184456  lr: 0.2
loss: 56.802270839  lr: 0.2
loss: 58.1246974091  lr: 0.2
loss: 56.4002349096  lr: 0.2
loss: 61.4444036396  lr: 0.2
loss: 63.0453043899  lr: 0.2
loss: 55.5336310444  lr: 0.2
loss: 56.6492668352  lr: 0.2
loss: 61.3693928947  lr: 0.2
loss: 58.8398858003  lr: 0.2
loss: 60.822662098  lr: 0.2
loss: 54.4092828665  lr: 0.2
loss: 63.7598566561  lr: 0.2
loss: 60.7164010181  lr: 0.2
loss: 58.5802595864  lr: 0.2
loss: 55.6634922414  lr: 0.2
loss: 55.4605248793  lr: 0.2
loss: 50.8699237585  lr: 0.2
loss: 54.5036661631  lr: 0.2
loss: 58.8707245844  lr: 0.2
loss: 60.0203948894  lr: 0.2
loss: 55.2784935101  lr: 0.2
loss: 60.4361541983  lr: 0.2
loss: 62.9376666876  lr: 0.2
loss: 60.6774197239  lr: 0.2
loss: 63.0918898503  lr: 0.2
loss: 57.2505154977  lr: 0.2
loss: 58.6636068261  lr: 0.2
loss: 61.9884687504  lr: 0.2
loss: 62.5497724525  lr: 0.2
loss: 56.5817002268  lr: 0.2
loss: 57.2768809796  lr: 0.2
loss: 57.3269477508  lr: 0.2
loss: 56.7718950015  lr: 0.2
loss: 55.3974013968  lr: 0.2
loss: 64.3686997243  lr: 0.2
loss: 62.2331526891  lr: 0.2
loss: 56.3984398672  lr: 0.2
loss: 60.4914755107  lr: 0.2
loss: 55.8423042592  lr: 0.2
loss: 55.5797889507  lr: 0.2
loss: 50.2343464635  lr: 0.2
loss: 62.346182325  lr: 0.2
loss: 54.638768685  lr: 0.2
loss: 60.9029564503  lr: 0.2
loss: 58.1850324832  lr: 0.2
loss: 57.4490749793  lr: 0.2
loss: 55.183257592  lr: 0.2
loss: 61.0129047951  lr: 0.2
loss: 61.1885124254  lr: 0.2
loss: 55.2735236398  lr: 0.2
loss: 58.7027930068  lr: 0.2

Correct: 16000 out of 16000 (1.0)

