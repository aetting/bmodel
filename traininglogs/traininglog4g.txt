Training set: trainingpairs-br-orig-ster0.25-dutch
Training code: full
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: False
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: try .25 of ster data -- now with constant lr

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 130.246428989  lr: 0.2
loss: 114.891790759  lr: 0.2
loss: 107.828422965  lr: 0.2
loss: 98.2655950291  lr: 0.2
loss: 97.6302266908  lr: 0.2
loss: 92.0898820655  lr: 0.2
loss: 88.2734279206  lr: 0.2
loss: 80.2129667545  lr: 0.2
loss: 82.4625630601  lr: 0.2
loss: 81.3027106657  lr: 0.2
loss: 80.525969109  lr: 0.2
loss: 80.8863189991  lr: 0.2
loss: 80.3312927897  lr: 0.2
loss: 81.948506411  lr: 0.2
loss: 75.4127829265  lr: 0.2
loss: 77.5779577647  lr: 0.2
loss: 77.7811124414  lr: 0.2
loss: 78.9655826674  lr: 0.2
loss: 77.6097942279  lr: 0.2
loss: 79.8372574884  lr: 0.2
loss: 77.8750870675  lr: 0.2
loss: 78.6242467197  lr: 0.2
loss: 79.5635232513  lr: 0.2
loss: 78.6104608051  lr: 0.2
loss: 80.9120259838  lr: 0.2
loss: 75.9284276432  lr: 0.2
loss: 79.7381751174  lr: 0.2
loss: 76.696971266  lr: 0.2
loss: 77.3179521468  lr: 0.2
loss: 77.2442598963  lr: 0.2
loss: 78.5113236179  lr: 0.2
loss: 78.4007437989  lr: 0.2
loss: 79.7095321148  lr: 0.2
loss: 79.0396998811  lr: 0.2
loss: 77.4214984095  lr: 0.2
loss: 78.4157546981  lr: 0.2
loss: 81.4275266022  lr: 0.2
loss: 78.7852775145  lr: 0.2
loss: 80.6091144554  lr: 0.2
loss: 75.8826898813  lr: 0.2
loss: 78.8295457352  lr: 0.2
loss: 79.0166434325  lr: 0.2
loss: 79.6543860648  lr: 0.2
loss: 77.3181645359  lr: 0.2
loss: 79.4125155425  lr: 0.2
loss: 77.9250519008  lr: 0.2
loss: 79.116121862  lr: 0.2
loss: 75.8411698303  lr: 0.2
loss: 78.016425049  lr: 0.2
loss: 80.3407496741  lr: 0.2
loss: 77.7534451782  lr: 0.2
loss: 77.4379693478  lr: 0.2
loss: 77.9951576869  lr: 0.2
loss: 78.0697850843  lr: 0.2
loss: 81.3469732002  lr: 0.2
loss: 77.1057940288  lr: 0.2
loss: 80.1797926308  lr: 0.2
loss: 77.0455605953  lr: 0.2
loss: 78.6938815832  lr: 0.2
loss: 75.9742568247  lr: 0.2
loss: 74.8623204206  lr: 0.2
loss: 77.2680081824  lr: 0.2
loss: 78.9483490102  lr: 0.2
loss: 77.1530734758  lr: 0.2
loss: 74.780012749  lr: 0.2
loss: 77.5342271083  lr: 0.2
loss: 79.1445434879  lr: 0.2
loss: 76.2464658491  lr: 0.2
loss: 77.2157979256  lr: 0.2
loss: 79.4778157208  lr: 0.2

TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 158.110849783  lr: 0.2
loss: 153.449228108  lr: 0.2
loss: 152.104068622  lr: 0.2
loss: 148.800422609  lr: 0.2
loss: 146.072701573  lr: 0.2
loss: 143.412284592  lr: 0.2
loss: 121.236231798  lr: 0.2
loss: 107.21537121  lr: 0.2
loss: 92.1609353269  lr: 0.2
loss: 81.9379482923  lr: 0.2
loss: 77.0123892637  lr: 0.2
loss: 78.5508790104  lr: 0.2
loss: 79.6119676572  lr: 0.2
loss: 77.34788376  lr: 0.2
loss: 79.5708530437  lr: 0.2
loss: 81.2630050759  lr: 0.2
loss: 78.4561484921  lr: 0.2
loss: 79.0162445122  lr: 0.2
loss: 74.0968448713  lr: 0.2
loss: 77.0481886519  lr: 0.2
loss: 78.3447024922  lr: 0.2
loss: 79.3316500804  lr: 0.2
loss: 78.3218673781  lr: 0.2
loss: 75.0800128072  lr: 0.2
loss: 78.0985943305  lr: 0.2
loss: 78.0412527757  lr: 0.2
loss: 76.7995555802  lr: 0.2
loss: 76.8389921572  lr: 0.2
loss: 77.3226681716  lr: 0.2
loss: 79.9771768108  lr: 0.2
loss: 75.2379544486  lr: 0.2
loss: 76.4147944389  lr: 0.2
loss: 79.8602683798  lr: 0.2
loss: 78.2625924099  lr: 0.2
loss: 76.9975363606  lr: 0.2
loss: 79.6935451614  lr: 0.2
loss: 79.7641068565  lr: 0.2
loss: 77.5953741459  lr: 0.2
loss: 75.9326561618  lr: 0.2
loss: 77.1085459678  lr: 0.2
loss: 77.4788858899  lr: 0.2
loss: 76.9611261345  lr: 0.2
loss: 78.9646516041  lr: 0.2
loss: 76.9180130682  lr: 0.2
loss: 76.1596006943  lr: 0.2
loss: 76.1367148632  lr: 0.2
loss: 75.8288452795  lr: 0.2
loss: 77.5878730018  lr: 0.2
loss: 76.9499516518  lr: 0.2
loss: 74.3284186943  lr: 0.2
loss: 75.1368639911  lr: 0.2
loss: 75.7861248401  lr: 0.2
loss: 76.2496255154  lr: 0.2
loss: 77.5343300187  lr: 0.2
loss: 76.2296082841  lr: 0.2
loss: 76.6567003275  lr: 0.2
loss: 78.8157954174  lr: 0.2
loss: 76.7975656841  lr: 0.2
loss: 78.1505584022  lr: 0.2
loss: 76.471115642  lr: 0.2
loss: 75.6381386157  lr: 0.2
loss: 74.7074083966  lr: 0.2
loss: 79.4202094832  lr: 0.2
loss: 77.7974969961  lr: 0.2
loss: 75.3255381996  lr: 0.2
loss: 76.4818605218  lr: 0.2
loss: 78.2020296664  lr: 0.2
loss: 80.1003037382  lr: 0.2
loss: 73.8038753878  lr: 0.2
loss: 71.7698635823  lr: 0.2
lastloss: 71.7698635823 lr: 0.2 update: 7000

Correct: 10000 out of 10000 (1.0)

