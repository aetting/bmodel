TRAINING PART ONE


NetInteg (
  (integ): Linear (250 -> 200)
  (integ_out): Linear (200 -> 150)
)

loss: 206.858676091

loss: 95.2954033883

loss: 85.9454339561

loss: 77.5404154995

loss: 74.5496347764

loss: 69.058348618

loss: 67.4368131022

loss: 66.8565171707

loss: 64.4595853975

loss: 59.0351698581

loss: 59.6675737811

loss: 61.5776457137

loss: 60.2593629614

loss: 63.6009313308

loss: 71.6285027375

loss: 54.8270715017

loss: 66.5798666513

loss: 53.2210919207

loss: 54.4352528893

loss: 57.5283292057

loss: 55.939522071

loss: 57.9581633011

loss: 60.5166106633

loss: 63.3151287766

loss: 50.3912319128

loss: 66.1566051877

loss: 64.1369426084

loss: 62.5096077233

loss: 54.056686292

loss: 59.9412483986

loss: 60.2810901224

loss: 58.9483585177

loss: 59.122251046

loss: 56.6250709859

loss: 57.7700194241

loss: 57.3514595928

loss: 54.4606117657

loss: 60.4989342573

loss: 57.9684963582

loss: 57.9881796482

loss: 65.3190977346

loss: 58.6938416382

loss: 60.0543442358

loss: 61.476742242

loss: 60.3996934137

loss: 60.8176257766

loss: 57.7956156556

loss: 60.1175841182

loss: 57.7617083288

loss: 61.9537164014

loss: 55.9383261693

loss: 59.8497620586

loss: 59.010862013

loss: 47.5370563576

loss: 58.5845535506

loss: 63.4344346519

loss: 60.1454158885

loss: 50.8377234484

loss: 63.18735887

loss: 56.1987442988

loss: 55.6418563414

loss: 62.0945330544

loss: 56.9777019626

loss: 63.2011062407

loss: 59.1170453966

loss: 54.0681801925

loss: 60.3210064803

loss: 61.5738693147

loss: 53.8175453416

loss: 62.5060386989

Correct: 16000 out of 16000 (1.0)
TRAINING PART TWO


NetFull (
  (retr): Linear (234 -> 80)
  (retr_out): Linear (80 -> 50)
  (integ): Linear (250 -> 200)
  (integ_out): Linear (200 -> 150)
)

loss: 181.210079193

loss: 141.218549386

loss: 120.014385343

loss: 92.3294141726

loss: 82.6945387563

loss: 65.2138471382

loss: 69.5776345797

loss: 56.2947372339

loss: 63.4744278804

loss: 57.7531041924

loss: 61.1373563228

loss: 62.1318303393

loss: 54.5031181918

loss: 56.953318797

loss: 58.0476402384

loss: 59.089586604

loss: 64.0421261852

loss: 61.0268372873

loss: 59.8816971219

loss: 59.5182515837

loss: 59.2577130663

loss: 55.3264755535

loss: 55.7968934258

loss: 62.862145081

loss: 57.0750435548

loss: 56.1581490789

loss: 53.8659478715

loss: 62.9335120216

loss: 55.3600368958

loss: 59.7004604045

loss: 49.0668435101

loss: 57.5866231618

loss: 64.0596628396

loss: 57.5854597109

loss: 57.8432442854

loss: 63.8704494508

loss: 65.2874029139

loss: 57.5526798192

loss: 61.4804562441

loss: 54.8685085188

loss: 55.5570868925

loss: 60.041038205

loss: 64.3840200733

loss: 61.49581568

loss: 59.4944852605

loss: 56.310583166

loss: 58.7789772306

loss: 61.9347755435

loss: 59.9291270571

loss: 55.726439599

loss: 58.0167435506

loss: 59.6028247249

loss: 57.3841536457

loss: 58.9464090269

loss: 60.1806148589

loss: 52.837003394

loss: 64.7729121631

loss: 55.961465061

loss: 59.7944818754

loss: 61.1444851872

loss: 54.4972211442

loss: 60.1681564992

loss: 60.3908262441

loss: 61.334757365

loss: 57.8309661235

loss: 59.4288879288

loss: 55.0971872676

loss: 62.1942105538

loss: 51.9923075925

loss: 56.7601406118

Correct: 16000 out of 16000 (1.0)
