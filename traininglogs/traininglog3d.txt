Training set: trainingpairs-br-sterdutch8k
Training code: p2
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: False
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: full model only training with stereotypical only -- now with full 7000 epochs to check


TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 132.154572621  lr: 0.2
loss: 133.52899459  lr: 0.2
loss: 132.639272034  lr: 0.2
loss: 120.174065784  lr: 0.2
loss: 94.5297634378  lr: 0.2
loss: 85.0547437742  lr: 0.2
loss: 70.9199248105  lr: 0.2
loss: 57.472658095  lr: 0.2
loss: 86.7450611461  lr: 0.2
loss: 34.4086511542  lr: 0.2
loss: 65.4401618305  lr: 0.2
loss: 25.8543640916  lr: 0.2
loss: 21.6781238943  lr: 0.2
loss: 21.084872118  lr: 0.2
loss: 20.9614261263  lr: 0.2
loss: 20.9249734935  lr: 0.2
loss: 20.8455202026  lr: 0.2
loss: 20.6181247412  lr: 0.2
loss: 20.4276170203  lr: 0.2
loss: 19.9488402109  lr: 0.2
loss: 20.4165983559  lr: 0.2
loss: 19.9329518194  lr: 0.2
loss: 19.893199906  lr: 0.2
loss: 20.1457979322  lr: 0.2
loss: 20.1328626571  lr: 0.2
loss: 20.0514475925  lr: 0.2
loss: 20.2617858864  lr: 0.2
loss: 19.821534595  lr: 0.2
loss: 20.1777697961  lr: 0.2
loss: 19.9640272456  lr: 0.2
loss: 20.3240675597  lr: 0.2
loss: 19.9488592861  lr: 0.2
loss: 20.3249015723  lr: 0.2
loss: 19.7688732367  lr: 0.2
loss: 20.2453709858  lr: 0.2
loss: 19.8909615411  lr: 0.2
loss: 19.6856922032  lr: 0.2
loss: 20.4359943626  lr: 0.2
loss: 19.7906274957  lr: 0.2
loss: 19.938518333  lr: 0.2
loss: 20.0850261086  lr: 0.2
loss: 20.0968675955  lr: 0.2
loss: 20.2793746153  lr: 0.2
loss: 19.8307836922  lr: 0.2
loss: 19.7731921292  lr: 0.2
loss: 19.5948816618  lr: 0.2
loss: 19.9923451675  lr: 0.2
loss: 19.7454335627  lr: 0.2
loss: 19.9768719195  lr: 0.2
loss: 20.0824903146  lr: 0.2
loss: 19.8815253794  lr: 0.2
loss: 20.1347590898  lr: 0.2
loss: 20.0767784942  lr: 0.2
loss: 19.9377670658  lr: 0.2
loss: 19.7820880085  lr: 0.2
loss: 20.1617602076  lr: 0.2
loss: 19.9386725066  lr: 0.2
loss: 19.9775691971  lr: 0.2
loss: 19.9898828084  lr: 0.2
loss: 19.9780032642  lr: 0.2
loss: 19.9486499507  lr: 0.2
loss: 20.4178606097  lr: 0.2
loss: 20.1851978376  lr: 0.2
loss: 19.9906205913  lr: 0.2
loss: 19.9034922111  lr: 0.2
loss: 19.7398181929  lr: 0.2
loss: 20.1709453744  lr: 0.2
loss: 19.9463669618  lr: 0.2
loss: 19.9022048274  lr: 0.2
loss: 20.0117012483  lr: 0.2
lastloss: 19.8809237781 lr: 0.2 update: 7001

Correct: 8000 out of 8000 (1.0)

