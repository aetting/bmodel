Training set: trainingpairs-br-holdoutB-dutch
Training code: full
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: True
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: train on holdout data B (even halves), this time with reducing lr in case it helps the erratic loss

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 106.117220614  lr: 0.2
loss: 97.2105159028  lr: 0.2
loss: 77.5915566161  lr: 0.2
loss: 76.0745785572  lr: 0.2
loss: 72.4567949682  lr: 0.2
loss: 74.2545732697  lr: 0.2
loss: 67.4135818825  lr: 0.2
loss: 63.0525754077  lr: 0.19
loss: 59.0862612385  lr: 0.19
loss: 59.3280225433  lr: 0.19
loss: 52.5033113649  lr: 0.19
loss: 59.8251734106  lr: 0.19
loss: 55.9298740322  lr: 0.19
loss: 53.4882480908  lr: 0.19
loss: 58.7820565648  lr: 0.1805
loss: 55.0206656289  lr: 0.1805
loss: 59.3452658441  lr: 0.1805
loss: 54.6479161944  lr: 0.1805
loss: 65.6137258969  lr: 0.1805
loss: 62.2222125088  lr: 0.1805
loss: 62.1272365904  lr: 0.1805
loss: 65.2956845938  lr: 0.171475
loss: 59.0580188241  lr: 0.171475
loss: 57.3326455811  lr: 0.171475
loss: 62.2859982058  lr: 0.171475
loss: 57.0048832306  lr: 0.171475
loss: 58.4664506557  lr: 0.171475
loss: 53.9845325378  lr: 0.171475
loss: 55.960497186  lr: 0.16290125
loss: 50.4958108069  lr: 0.16290125
loss: 61.3990264299  lr: 0.16290125
loss: 57.6339142147  lr: 0.16290125
loss: 53.4482068502  lr: 0.16290125
loss: 57.3796212873  lr: 0.16290125
loss: 58.5355589002  lr: 0.16290125
loss: 55.2755434341  lr: 0.1547561875
loss: 58.0685603588  lr: 0.1547561875
loss: 56.6291350026  lr: 0.1547561875
loss: 58.1962684923  lr: 0.1547561875
loss: 57.6403664989  lr: 0.1547561875
loss: 52.7335440894  lr: 0.1547561875
loss: 61.9421269694  lr: 0.1547561875
loss: 53.4075764681  lr: 0.147018378125
loss: 55.7092948738  lr: 0.147018378125
loss: 59.4773062146  lr: 0.147018378125
loss: 61.5812252664  lr: 0.147018378125
loss: 60.1260892658  lr: 0.147018378125
loss: 53.2607682105  lr: 0.147018378125
loss: 50.864824465  lr: 0.147018378125
loss: 56.9380289953  lr: 0.139667459219
loss: 49.4611148208  lr: 0.139667459219
loss: 59.8108389518  lr: 0.139667459219
loss: 63.5849673761  lr: 0.139667459219
loss: 58.6921313874  lr: 0.139667459219
loss: 58.2539625357  lr: 0.139667459219
loss: 54.9098832379  lr: 0.139667459219
loss: 63.0144900195  lr: 0.132684086258
loss: 55.7686318863  lr: 0.132684086258
loss: 55.6617936248  lr: 0.132684086258
loss: 59.6876921675  lr: 0.132684086258
loss: 59.2891351227  lr: 0.132684086258
loss: 52.8735544074  lr: 0.132684086258
loss: 54.2172708474  lr: 0.132684086258
loss: 60.662381016  lr: 0.126049881945
loss: 56.3184743398  lr: 0.126049881945
loss: 60.8799356686  lr: 0.126049881945
loss: 56.2260505583  lr: 0.126049881945
loss: 52.9026177498  lr: 0.126049881945
loss: 58.3643441914  lr: 0.126049881945
loss: 53.2742285543  lr: 0.126049881945
lastloss: 53.2742285543 lr: 0.119747387848 update: 7000

Correct: 15840 out of 15840 (1.0)



TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 138.861442208  lr: 0.2
loss: 140.035457864  lr: 0.2
loss: 130.903061405  lr: 0.2
loss: 99.4327263306  lr: 0.2
loss: 73.5504013696  lr: 0.2
loss: 63.5151397073  lr: 0.2
loss: 63.5532806581  lr: 0.2
loss: 53.5917039311  lr: 0.19
loss: 56.8125216799  lr: 0.19
loss: 52.7130621118  lr: 0.19
loss: 53.9551953222  lr: 0.19
loss: 55.5183515084  lr: 0.19
loss: 53.4718258848  lr: 0.19
loss: 54.7748433812  lr: 0.19
loss: 62.7549430201  lr: 0.1805
loss: 62.4442610659  lr: 0.1805
loss: 59.5564721845  lr: 0.1805
loss: 54.2938581497  lr: 0.1805
loss: 56.9642575701  lr: 0.1805
loss: 50.8821491695  lr: 0.1805
loss: 62.2091941525  lr: 0.1805
loss: 55.1915381422  lr: 0.171475
loss: 56.8467710094  lr: 0.171475
loss: 55.8220416024  lr: 0.171475
loss: 58.0616779714  lr: 0.171475
loss: 52.8611701804  lr: 0.171475
loss: 52.2548184056  lr: 0.171475
loss: 57.2795340616  lr: 0.171475
loss: 56.0487814193  lr: 0.16290125
loss: 51.6554070214  lr: 0.16290125
loss: 50.8830853716  lr: 0.16290125
loss: 58.098330676  lr: 0.16290125
loss: 59.425906725  lr: 0.16290125
loss: 57.3385641068  lr: 0.16290125
loss: 62.6567590504  lr: 0.16290125
loss: 61.4917315944  lr: 0.1547561875
loss: 57.6914230787  lr: 0.1547561875
loss: 52.6827091816  lr: 0.1547561875
loss: 54.7177421628  lr: 0.1547561875
loss: 60.2138020591  lr: 0.1547561875
loss: 58.7092013903  lr: 0.1547561875
loss: 59.7824449185  lr: 0.1547561875
loss: 53.1020540515  lr: 0.147018378125
loss: 54.8457180991  lr: 0.147018378125
loss: 57.7946828094  lr: 0.147018378125
loss: 60.7434254289  lr: 0.147018378125
loss: 59.5102152584  lr: 0.147018378125
loss: 46.3557327972  lr: 0.147018378125
loss: 58.807711052  lr: 0.147018378125
loss: 57.3384925021  lr: 0.139667459219
loss: 54.3198661227  lr: 0.139667459219
loss: 50.6184681434  lr: 0.139667459219
loss: 56.7759185681  lr: 0.139667459219
loss: 58.6850875341  lr: 0.139667459219
loss: 53.6621505581  lr: 0.139667459219
loss: 54.1189852378  lr: 0.139667459219
loss: 54.7464091488  lr: 0.132684086258
loss: 54.5375568373  lr: 0.132684086258
loss: 55.1300122162  lr: 0.132684086258
loss: 57.0450196843  lr: 0.132684086258
loss: 58.253261652  lr: 0.132684086258
loss: 59.9761415045  lr: 0.132684086258
loss: 58.1433871505  lr: 0.132684086258
loss: 53.249968608  lr: 0.126049881945
loss: 56.7468459681  lr: 0.126049881945
loss: 60.4383497488  lr: 0.126049881945
loss: 57.5862357952  lr: 0.126049881945
loss: 58.5007858691  lr: 0.126049881945
loss: 52.8006391613  lr: 0.126049881945
loss: 58.6156835297  lr: 0.126049881945
lastloss: 50.640718002 lr: 0.119747387848 update: 7001

Correct: 15840 out of 15840 (1.0)

