Training set: trainingpairs-br-origfulldutch
Embeddings: brouwerCOALS-100.txt
Binary: True
Reduce lr: True
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: Trying reducing learning rate with same settings once more to see how much variation there is from one run to another

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 116.52817465  lr: 0.2
loss: 99.2126755249  lr: 0.2
loss: 82.1748949853  lr: 0.2
loss: 80.1272634652  lr: 0.2
loss: 78.2322926358  lr: 0.2
loss: 75.0744201589  lr: 0.2
loss: 76.8719096947  lr: 0.2
loss: 76.3162797585  lr: 0.19
loss: 73.6866308034  lr: 0.19
loss: 73.6531261823  lr: 0.19
loss: 59.7414292825  lr: 0.19
loss: 64.2974235171  lr: 0.19
loss: 68.5371807827  lr: 0.19
loss: 71.1229757631  lr: 0.19
loss: 62.3484902859  lr: 0.1805
loss: 64.1547563542  lr: 0.1805
loss: 63.0127438843  lr: 0.1805
loss: 67.3171921103  lr: 0.1805
loss: 64.5572531711  lr: 0.1805
loss: 62.3889355822  lr: 0.1805
loss: 61.7308567097  lr: 0.1805
loss: 65.5527153639  lr: 0.171475
loss: 64.1971408819  lr: 0.171475
loss: 59.4970298757  lr: 0.171475
loss: 66.0735544461  lr: 0.171475
loss: 63.8547453568  lr: 0.171475
loss: 66.3767002791  lr: 0.171475
loss: 64.1702287699  lr: 0.171475
loss: 59.0175561771  lr: 0.16290125
loss: 62.1447857351  lr: 0.16290125
loss: 62.9352767827  lr: 0.16290125
loss: 62.4305488903  lr: 0.16290125
loss: 63.4176927021  lr: 0.16290125
loss: 66.4619906363  lr: 0.16290125
loss: 57.2837991386  lr: 0.16290125
loss: 60.1392842786  lr: 0.1547561875
loss: 61.7118823152  lr: 0.1547561875
loss: 65.2072432014  lr: 0.1547561875
loss: 59.3371566611  lr: 0.1547561875
loss: 64.0792127756  lr: 0.1547561875
loss: 61.7212534582  lr: 0.1547561875
loss: 60.3926505965  lr: 0.1547561875
loss: 60.4156695708  lr: 0.147018378125
loss: 59.685561524  lr: 0.147018378125
loss: 59.6898521324  lr: 0.147018378125
loss: 56.45511845  lr: 0.147018378125
loss: 60.0975586052  lr: 0.147018378125
loss: 62.3174352023  lr: 0.147018378125
loss: 60.2291451282  lr: 0.147018378125
loss: 68.0969614102  lr: 0.139667459219
loss: 58.0124895263  lr: 0.139667459219
loss: 64.9940016589  lr: 0.139667459219
loss: 63.2383544871  lr: 0.139667459219
loss: 58.5422798384  lr: 0.139667459219
loss: 65.0933976141  lr: 0.139667459219
loss: 69.50958929  lr: 0.139667459219
loss: 57.5303491933  lr: 0.132684086258
loss: 64.5438059495  lr: 0.132684086258
loss: 63.4370705807  lr: 0.132684086258
loss: 65.2463841769  lr: 0.132684086258
loss: 63.6934651889  lr: 0.132684086258
loss: 59.4933690349  lr: 0.132684086258
loss: 61.6463081839  lr: 0.132684086258
loss: 63.2800294149  lr: 0.126049881945
loss: 61.2785066101  lr: 0.126049881945
loss: 61.2974204801  lr: 0.126049881945
loss: 62.753704868  lr: 0.126049881945
loss: 59.8503596549  lr: 0.126049881945
loss: 58.9162046429  lr: 0.126049881945
loss: 57.7904933919  lr: 0.126049881945

Correct: 16000 out of 16000 (1.0)

TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 155.21321696  lr: 0.2
loss: 141.674020555  lr: 0.2
loss: 89.9735826435  lr: 0.2
loss: 76.9929758215  lr: 0.2
loss: 73.9036137347  lr: 0.2
loss: 69.8540176591  lr: 0.2
loss: 61.1875400451  lr: 0.2
loss: 64.5149958233  lr: 0.19
loss: 61.0892735544  lr: 0.19
loss: 66.6314669603  lr: 0.19
loss: 63.5752163631  lr: 0.19
loss: 59.5619467035  lr: 0.19
loss: 61.4706582179  lr: 0.19
loss: 66.4946261949  lr: 0.19
loss: 65.0563116867  lr: 0.1805
loss: 60.824750605  lr: 0.1805
loss: 58.9036332328  lr: 0.1805
loss: 66.1691025466  lr: 0.1805
loss: 59.9686675438  lr: 0.1805
loss: 64.3848422  lr: 0.1805
loss: 63.7701729397  lr: 0.1805
loss: 61.0013331341  lr: 0.171475
loss: 59.0309954352  lr: 0.171475
loss: 59.0469766343  lr: 0.171475
loss: 64.3847752081  lr: 0.171475
loss: 54.1576714364  lr: 0.171475
loss: 55.9401246333  lr: 0.171475
loss: 68.3192154117  lr: 0.171475
loss: 61.9807006463  lr: 0.16290125
loss: 64.1650882596  lr: 0.16290125
loss: 63.3359963277  lr: 0.16290125
loss: 60.1502417946  lr: 0.16290125
loss: 58.0343564678  lr: 0.16290125
loss: 65.1473165823  lr: 0.16290125
loss: 61.916368677  lr: 0.16290125
loss: 61.6989892183  lr: 0.1547561875
loss: 59.8826406352  lr: 0.1547561875
loss: 66.2383661138  lr: 0.1547561875
loss: 56.5990630391  lr: 0.1547561875
loss: 61.5080281065  lr: 0.1547561875
loss: 60.9967761073  lr: 0.1547561875
loss: 58.5962734786  lr: 0.1547561875
loss: 62.299769006  lr: 0.147018378125
loss: 59.5908316867  lr: 0.147018378125
loss: 65.5860948637  lr: 0.147018378125
loss: 58.4667431819  lr: 0.147018378125
loss: 66.1548236273  lr: 0.147018378125
loss: 64.242048033  lr: 0.147018378125
loss: 58.9136967583  lr: 0.147018378125
loss: 59.4569661125  lr: 0.139667459219
loss: 56.645618241  lr: 0.139667459219
loss: 65.8611837081  lr: 0.139667459219
loss: 65.2166976374  lr: 0.139667459219
loss: 56.5454686207  lr: 0.139667459219
loss: 60.1338557782  lr: 0.139667459219
loss: 56.0084549966  lr: 0.139667459219
loss: 59.2675458089  lr: 0.132684086258
loss: 62.8863486442  lr: 0.132684086258
loss: 60.1339148346  lr: 0.132684086258
loss: 56.7144413655  lr: 0.132684086258
loss: 63.3963767157  lr: 0.132684086258
loss: 58.1347286194  lr: 0.132684086258
loss: 63.1500748686  lr: 0.132684086258
loss: 64.6817124617  lr: 0.126049881945
loss: 68.4722704371  lr: 0.126049881945
loss: 61.1049838243  lr: 0.126049881945
loss: 53.6313619781  lr: 0.126049881945
loss: 60.9139698036  lr: 0.126049881945
loss: 62.0306632569  lr: 0.126049881945
loss: 63.8653449792  lr: 0.126049881945

Correct: 16000 out of 16000 (1.0)

