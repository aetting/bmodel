Training set: trainingpairs-br-orig-ster0.5-dutch
Training code: full
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: False
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: try .5 of ster data to test effect of ratio

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 125.746972546  lr: 0.2
loss: 106.600956207  lr: 0.2
loss: 88.7460949384  lr: 0.2
loss: 95.8985294281  lr: 0.2
loss: 92.7534913912  lr: 0.2
loss: 83.137740708  lr: 0.2
loss: 79.3108085755  lr: 0.2
loss: 85.0422170883  lr: 0.2
loss: 76.3101989051  lr: 0.2
loss: 73.0333767789  lr: 0.2
loss: 76.0430077025  lr: 0.2
loss: 74.9577718679  lr: 0.2
loss: 73.1226378423  lr: 0.2
loss: 75.2743764715  lr: 0.2
loss: 74.3641164754  lr: 0.2
loss: 74.3740736122  lr: 0.2
loss: 74.119265376  lr: 0.2
loss: 75.9299480527  lr: 0.2
loss: 75.7143601074  lr: 0.2
loss: 70.774785846  lr: 0.2
loss: 68.4534732647  lr: 0.2
loss: 70.1188671103  lr: 0.2
loss: 73.9339391174  lr: 0.2
loss: 70.0157647343  lr: 0.2
loss: 73.7208002402  lr: 0.2
loss: 73.4362239027  lr: 0.2
loss: 75.2785630953  lr: 0.2
loss: 72.1593736907  lr: 0.2
loss: 71.6869279317  lr: 0.2
loss: 68.6468931305  lr: 0.2
loss: 69.7327038865  lr: 0.2
loss: 70.9429876228  lr: 0.2
loss: 71.9672520445  lr: 0.2
loss: 69.5022070786  lr: 0.2
loss: 72.2570462636  lr: 0.2
loss: 67.6555915685  lr: 0.2
loss: 70.6377970077  lr: 0.2
loss: 69.6385377375  lr: 0.2
loss: 73.3800443196  lr: 0.2
loss: 67.9433950328  lr: 0.2
loss: 73.1153247675  lr: 0.2
loss: 65.3775091184  lr: 0.2
loss: 72.4679305958  lr: 0.2
loss: 73.4547699763  lr: 0.2
loss: 68.7721144571  lr: 0.2
loss: 71.6714415616  lr: 0.2
loss: 73.328397508  lr: 0.2
loss: 72.466546436  lr: 0.2
loss: 71.0049001995  lr: 0.2
loss: 71.2448208654  lr: 0.2
loss: 66.8677913466  lr: 0.2
loss: 72.7086742791  lr: 0.2
loss: 69.812904058  lr: 0.2
loss: 70.8346458103  lr: 0.2
loss: 71.5892021549  lr: 0.2
loss: 68.2855594905  lr: 0.2
loss: 73.576179614  lr: 0.2
loss: 72.4411956881  lr: 0.2
loss: 70.2499786698  lr: 0.2
loss: 73.8780896866  lr: 0.2
loss: 71.0661628865  lr: 0.2
loss: 75.0561772488  lr: 0.2
loss: 67.5733058523  lr: 0.2
loss: 69.6505272442  lr: 0.2
loss: 73.472327867  lr: 0.2
loss: 69.9882174821  lr: 0.2
loss: 72.9828146653  lr: 0.2
loss: 64.959406094  lr: 0.2
loss: 71.5883717527  lr: 0.2
loss: 72.6762470593  lr: 0.2
lastloss: 69.9278082266 lr: 0.2 update: 7001

Correct: 12000 out of 12000 (1.0)



TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 152.246106178  lr: 0.2
loss: 147.552636281  lr: 0.2
loss: 139.211937971  lr: 0.2
loss: 129.312159488  lr: 0.2
loss: 107.476599025  lr: 0.2
loss: 89.9205168609  lr: 0.2
loss: 75.0602621007  lr: 0.2
loss: 77.0384005225  lr: 0.2
loss: 73.3023098197  lr: 0.2
loss: 73.8746345395  lr: 0.2
loss: 72.7220309927  lr: 0.2
loss: 71.7732993227  lr: 0.2
loss: 69.1394639806  lr: 0.2
loss: 74.7180410747  lr: 0.2
loss: 70.6252551841  lr: 0.2
loss: 72.6306784651  lr: 0.2
loss: 68.2803594333  lr: 0.2
loss: 76.1190392572  lr: 0.2
loss: 69.5643341862  lr: 0.2
loss: 70.3814463615  lr: 0.2
loss: 75.5231256359  lr: 0.2
loss: 70.1869736403  lr: 0.2
loss: 73.117472439  lr: 0.2
loss: 70.4503742338  lr: 0.2
loss: 72.740656871  lr: 0.2
loss: 68.4054435421  lr: 0.2
loss: 71.743662608  lr: 0.2
loss: 72.0213031869  lr: 0.2
loss: 73.1042881623  lr: 0.2
loss: 76.0723886806  lr: 0.2
loss: 70.9330994532  lr: 0.2
loss: 72.255916594  lr: 0.2
loss: 70.7913231555  lr: 0.2
loss: 74.1171691997  lr: 0.2
loss: 73.2212742208  lr: 0.2
loss: 68.1735152629  lr: 0.2
loss: 75.355064939  lr: 0.2
loss: 72.3190713266  lr: 0.2
loss: 68.4215185169  lr: 0.2
loss: 71.1283556752  lr: 0.2
loss: 72.8522139118  lr: 0.2
loss: 63.7275365952  lr: 0.2
loss: 74.1993799745  lr: 0.2
loss: 75.7613876714  lr: 0.2
loss: 70.3291845319  lr: 0.2
loss: 72.0038911211  lr: 0.2
loss: 73.5904730098  lr: 0.2
loss: 68.5661866505  lr: 0.2
loss: 74.32480791  lr: 0.2
loss: 70.3772392978  lr: 0.2
loss: 66.6538481876  lr: 0.2
loss: 73.181293541  lr: 0.2
loss: 69.8179824513  lr: 0.2
loss: 69.9142268823  lr: 0.2
loss: 77.1989817389  lr: 0.2
loss: 71.3410418864  lr: 0.2
loss: 73.8798153016  lr: 0.2
loss: 71.6954726214  lr: 0.2
loss: 69.9860464413  lr: 0.2
loss: 69.9025420556  lr: 0.2
loss: 70.8161200865  lr: 0.2
loss: 68.5718365697  lr: 0.2
loss: 71.9858432483  lr: 0.2
loss: 69.6403815027  lr: 0.2
loss: 75.6697673488  lr: 0.2
loss: 73.0244934125  lr: 0.2
loss: 66.775791833  lr: 0.2
loss: 75.6608333615  lr: 0.2
loss: 69.7097268644  lr: 0.2
loss: 68.2275015624  lr: 0.2
lastloss: 68.2275015624 lr: 0.2 update: 7000

Correct: 12000 out of 12000 (1.0)

