Training set: trainingpairs-br-nonsterdutch8k
Training code: p2
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: False
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: full model only training with NONstereotypical only


TRAINING PART TWO

100 items per update, 10000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 144.415015787  lr: 0.2
loss: 141.716816455  lr: 0.2
loss: 140.714360878  lr: 0.2
loss: 143.795400292  lr: 0.2
loss: 141.755335584  lr: 0.2
loss: 138.670731902  lr: 0.2
loss: 138.632809788  lr: 0.2
loss: 137.907681465  lr: 0.2
loss: 141.099092275  lr: 0.2
loss: 138.414525166  lr: 0.2
loss: 134.11886473  lr: 0.2
loss: 134.971210584  lr: 0.2
loss: 132.043003634  lr: 0.2
loss: 137.550674587  lr: 0.2
loss: 137.92321527  lr: 0.2
loss: 131.404271513  lr: 0.2
loss: 132.006094471  lr: 0.2
loss: 134.467977546  lr: 0.2
loss: 126.692890555  lr: 0.2
loss: 126.737944759  lr: 0.2
loss: 120.966507521  lr: 0.2
loss: 122.581497926  lr: 0.2
loss: 119.774455041  lr: 0.2
loss: 119.854837865  lr: 0.2
loss: 114.500822254  lr: 0.2
loss: 114.939052396  lr: 0.2
loss: 107.59334892  lr: 0.2
loss: 101.309918825  lr: 0.2
loss: 96.3267168198  lr: 0.2
loss: 91.6337311454  lr: 0.2
loss: 88.9260942447  lr: 0.2
loss: 86.1496800752  lr: 0.2
loss: 84.9965799984  lr: 0.2
loss: 83.7482838674  lr: 0.2
loss: 83.5847950533  lr: 0.2
loss: 83.011586949  lr: 0.2
loss: 83.1370125043  lr: 0.2
loss: 83.5745620053  lr: 0.2
loss: 82.9719462834  lr: 0.2
loss: 83.1674472372  lr: 0.2
loss: 82.3096387036  lr: 0.2
loss: 83.3294210508  lr: 0.2
loss: 82.1129693671  lr: 0.2
loss: 84.8727527221  lr: 0.2
loss: 83.3257864481  lr: 0.2
loss: 82.8566317326  lr: 0.2
loss: 81.7115117888  lr: 0.2
loss: 80.5803432148  lr: 0.2
loss: 80.5939536052  lr: 0.2
loss: 83.3650962513  lr: 0.2
loss: 81.1049946719  lr: 0.2
loss: 83.0101490204  lr: 0.2
loss: 81.2363370523  lr: 0.2
loss: 82.2061577231  lr: 0.2
loss: 81.487286173  lr: 0.2
loss: 83.1689388599  lr: 0.2
loss: 83.4413158769  lr: 0.2
loss: 81.1502293064  lr: 0.2
loss: 83.6015448401  lr: 0.2
loss: 83.5531916301  lr: 0.2
loss: 80.9460010466  lr: 0.2
loss: 82.7643602426  lr: 0.2
loss: 82.9749178705  lr: 0.2
loss: 82.1605397861  lr: 0.2
loss: 81.4144347167  lr: 0.2
loss: 81.1425288886  lr: 0.2
loss: 82.7759706197  lr: 0.2
loss: 82.6678316287  lr: 0.2
loss: 82.6149394341  lr: 0.2
loss: 81.8716298112  lr: 0.2
loss: 82.5359714901  lr: 0.2
loss: 82.7365846571  lr: 0.2
loss: 82.94534881  lr: 0.2
loss: 82.8002679611  lr: 0.2
loss: 82.1922360348  lr: 0.2
loss: 81.4786705017  lr: 0.2
loss: 82.3858239025  lr: 0.2
loss: 83.0355545941  lr: 0.2
loss: 83.2728311414  lr: 0.2
loss: 83.3230842175  lr: 0.2
loss: 81.8806334772  lr: 0.2
loss: 81.9979372979  lr: 0.2
loss: 81.89267551  lr: 0.2
loss: 82.2441982338  lr: 0.2
loss: 80.6115104313  lr: 0.2
loss: 82.1135934307  lr: 0.2
loss: 82.2984690841  lr: 0.2
loss: 82.6912480891  lr: 0.2
loss: 83.0355809471  lr: 0.2
loss: 81.9173278687  lr: 0.2
loss: 80.6508441011  lr: 0.2
loss: 82.8181224684  lr: 0.2
loss: 81.7915646248  lr: 0.2
loss: 81.7646444935  lr: 0.2
loss: 81.1887130213  lr: 0.2
loss: 83.9389638786  lr: 0.2
loss: 82.8113477421  lr: 0.2
loss: 83.6511845153  lr: 0.2
loss: 81.2880577023  lr: 0.2
loss: 81.5890453181  lr: 0.2
lastloss: 81.5890453181 lr: 0.2 update: 10000

Correct: 8000 out of 8000 (1.0)

