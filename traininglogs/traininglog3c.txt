Training set: trainingpairs-br-nonsterdutch8k
Training code: p2
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: True
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: full model only training with NONstereotypical only


TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 141.055909559  lr: 0.2
loss: 141.261352539  lr: 0.2
loss: 142.627647549  lr: 0.2
loss: 140.152108356  lr: 0.2
loss: 141.969698682  lr: 0.2
loss: 140.988199502  lr: 0.2
loss: 142.929256842  lr: 0.2
loss: 140.985904887  lr: 0.19
loss: 139.852253675  lr: 0.19
loss: 137.644806847  lr: 0.19
loss: 138.894773528  lr: 0.19
loss: 136.143295944  lr: 0.19
loss: 135.921921059  lr: 0.19
loss: 133.396308064  lr: 0.19
loss: 133.903669834  lr: 0.1805
loss: 135.603760764  lr: 0.1805
loss: 130.479450166  lr: 0.1805
loss: 131.768892586  lr: 0.1805
loss: 130.345575035  lr: 0.1805
loss: 132.209533557  lr: 0.1805
loss: 129.591644585  lr: 0.1805
loss: 123.112849236  lr: 0.171475
loss: 123.20164679  lr: 0.171475
loss: 120.032707207  lr: 0.171475
loss: 120.62331881  lr: 0.171475
loss: 118.626624592  lr: 0.171475
loss: 118.73064772  lr: 0.171475
loss: 111.780929979  lr: 0.171475
loss: 112.560919557  lr: 0.16290125
loss: 108.092508638  lr: 0.16290125
loss: 109.115315981  lr: 0.16290125
loss: 105.900992416  lr: 0.16290125
loss: 104.001376143  lr: 0.16290125
loss: 98.9118515607  lr: 0.16290125
loss: 95.3802679218  lr: 0.16290125
loss: 97.1593332421  lr: 0.1547561875
loss: 93.8239185121  lr: 0.1547561875
loss: 89.5519248666  lr: 0.1547561875
loss: 88.6676449445  lr: 0.1547561875
loss: 89.0208364748  lr: 0.1547561875
loss: 87.1934310808  lr: 0.1547561875
loss: 87.6372954273  lr: 0.1547561875
loss: 84.6958941331  lr: 0.147018378125
loss: 85.7526252912  lr: 0.147018378125
loss: 85.2309481136  lr: 0.147018378125
loss: 84.7524628155  lr: 0.147018378125
loss: 84.2034898173  lr: 0.147018378125
loss: 84.0737919066  lr: 0.147018378125
loss: 82.3724904117  lr: 0.147018378125
loss: 84.057293184  lr: 0.139667459219
loss: 82.6818909899  lr: 0.139667459219
loss: 83.5705131695  lr: 0.139667459219
loss: 82.787992597  lr: 0.139667459219
loss: 82.275449415  lr: 0.139667459219
loss: 83.5975826688  lr: 0.139667459219
loss: 81.8555159981  lr: 0.139667459219
loss: 82.598318685  lr: 0.132684086258
loss: 82.4240254139  lr: 0.132684086258
loss: 82.9128852687  lr: 0.132684086258
loss: 82.6209054891  lr: 0.132684086258
loss: 82.0291645456  lr: 0.132684086258
loss: 83.4970372746  lr: 0.132684086258
loss: 81.1716673389  lr: 0.132684086258
loss: 83.6581200027  lr: 0.126049881945
loss: 81.3391904174  lr: 0.126049881945
loss: 83.5144760155  lr: 0.126049881945
loss: 82.1270761415  lr: 0.126049881945
loss: 83.8412642198  lr: 0.126049881945
loss: 80.6689150398  lr: 0.126049881945
loss: 81.9013099033  lr: 0.126049881945

Correct: 7931 out of 8000 (0.991375)

