Training set: trainingpairs-br-origfulldutch
Training code: full
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: False
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: original replication settings but trying with word2loc as input to integration layers

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (235 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 139.789923668  lr: 0.2
loss: 118.265710913  lr: 0.2
loss: 102.997819007  lr: 0.2
loss: 93.5084831715  lr: 0.2
loss: 95.1212622342  lr: 0.2
loss: 88.301073669  lr: 0.2
loss: 96.0381907471  lr: 0.2
loss: 74.5311866259  lr: 0.2
loss: 83.7742767489  lr: 0.2
loss: 79.1442815702  lr: 0.2
loss: 76.1674633623  lr: 0.2
loss: 70.2809145745  lr: 0.2
loss: 71.5682206344  lr: 0.2
loss: 67.9617870379  lr: 0.2
loss: 72.026069806  lr: 0.2
loss: 73.4763406424  lr: 0.2
loss: 71.1018319403  lr: 0.2
loss: 63.2038262594  lr: 0.2
loss: 63.0554388102  lr: 0.2
loss: 63.4546858668  lr: 0.2
loss: 60.3131040595  lr: 0.2
loss: 64.0915470043  lr: 0.2
loss: 64.4453738674  lr: 0.2
loss: 68.0906126077  lr: 0.2
loss: 61.7854620978  lr: 0.2
loss: 60.5736848  lr: 0.2
loss: 61.9333231481  lr: 0.2
loss: 64.5851010778  lr: 0.2
loss: 58.6373868609  lr: 0.2
loss: 64.6052556003  lr: 0.2
loss: 61.1131525295  lr: 0.2
loss: 59.1145155262  lr: 0.2
loss: 59.0441029828  lr: 0.2
loss: 60.0473680794  lr: 0.2
loss: 66.4423902732  lr: 0.2
loss: 61.4308684235  lr: 0.2
loss: 64.4251409555  lr: 0.2
loss: 65.8615391503  lr: 0.2
loss: 60.2858581207  lr: 0.2
loss: 59.8145233047  lr: 0.2
loss: 67.0290535265  lr: 0.2
loss: 65.9664590793  lr: 0.2
loss: 61.7619938754  lr: 0.2
loss: 60.984447725  lr: 0.2
loss: 62.2774590685  lr: 0.2
loss: 62.632928785  lr: 0.2
loss: 62.609338934  lr: 0.2
loss: 64.4795852199  lr: 0.2
loss: 62.8132506274  lr: 0.2
loss: 62.3145717786  lr: 0.2
loss: 59.1784145906  lr: 0.2
loss: 64.5051793219  lr: 0.2
loss: 59.8835651028  lr: 0.2
loss: 62.375197398  lr: 0.2
loss: 61.9426904943  lr: 0.2
loss: 55.4844749613  lr: 0.2
loss: 58.8760802855  lr: 0.2
loss: 63.478479067  lr: 0.2
loss: 63.4884325324  lr: 0.2
loss: 68.7608936263  lr: 0.2
loss: 59.8919775575  lr: 0.2
loss: 55.0410965519  lr: 0.2
loss: 63.3071997315  lr: 0.2
loss: 66.2204366374  lr: 0.2
loss: 65.1064635805  lr: 0.2
loss: 60.4818669603  lr: 0.2
loss: 54.6787082699  lr: 0.2
loss: 65.1652947343  lr: 0.2
loss: 52.351949063  lr: 0.2
loss: 57.3648572236  lr: 0.2
lastloss: 57.3648572236 lr: 0.2 update: 7000

Correct: 16000 out of 16000 (1.0)



TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 35)
  (integ): Linear (235 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 145.68889338  lr: 0.2
loss: 105.436821991  lr: 0.2
loss: 69.2748740509  lr: 0.2
loss: 69.2025800193  lr: 0.2
loss: 55.1460434262  lr: 0.2
loss: 57.805028678  lr: 0.2
loss: 60.2127914497  lr: 0.2
loss: 63.0052803215  lr: 0.2
loss: 59.6511469122  lr: 0.2
loss: 61.7256056208  lr: 0.2
loss: 60.0424435579  lr: 0.2
loss: 59.7746552153  lr: 0.2
loss: 62.8836471337  lr: 0.2
loss: 56.5826161293  lr: 0.2
loss: 63.1685794698  lr: 0.2
loss: 56.8576664093  lr: 0.2
loss: 59.3320138248  lr: 0.2
loss: 61.6769608607  lr: 0.2
loss: 62.7128308735  lr: 0.2
loss: 62.2204196133  lr: 0.2
loss: 60.2811546476  lr: 0.2
loss: 64.7254560737  lr: 0.2
loss: 54.5490182258  lr: 0.2
loss: 64.0863318837  lr: 0.2
loss: 59.140293102  lr: 0.2
loss: 63.9180511726  lr: 0.2
loss: 62.6446540253  lr: 0.2
loss: 67.2915638247  lr: 0.2
loss: 62.4347869201  lr: 0.2
loss: 61.8075932785  lr: 0.2
loss: 61.0770311389  lr: 0.2
loss: 64.4193729169  lr: 0.2
loss: 63.4634143835  lr: 0.2
loss: 66.7778335663  lr: 0.2
loss: 55.7887618611  lr: 0.2
loss: 61.2416755781  lr: 0.2
loss: 63.0967180563  lr: 0.2
loss: 58.4317870001  lr: 0.2
loss: 65.0923989682  lr: 0.2
loss: 63.0967299625  lr: 0.2
loss: 61.3680391589  lr: 0.2
loss: 62.9786109721  lr: 0.2
loss: 67.0537110947  lr: 0.2
loss: 58.9966829032  lr: 0.2
loss: 63.5124209248  lr: 0.2
loss: 54.7154741507  lr: 0.2
loss: 60.6489915481  lr: 0.2
loss: 63.6421218874  lr: 0.2
loss: 68.6657125526  lr: 0.2
loss: 59.1466012295  lr: 0.2
loss: 60.3322946924  lr: 0.2
loss: 66.0199827599  lr: 0.2
loss: 64.7912786578  lr: 0.2
loss: 61.9794624135  lr: 0.2
loss: 60.3159202568  lr: 0.2
loss: 60.637309679  lr: 0.2
loss: 64.4383995518  lr: 0.2
loss: 58.7035308637  lr: 0.2
loss: 60.8612566776  lr: 0.2
loss: 60.5421036382  lr: 0.2
loss: 54.7391705056  lr: 0.2
loss: 52.7553277547  lr: 0.2
loss: 58.7255350543  lr: 0.2
loss: 60.7900545179  lr: 0.2
loss: 62.7084365266  lr: 0.2
loss: 63.0238396942  lr: 0.2
loss: 66.7991038342  lr: 0.2
loss: 63.1187152906  lr: 0.2
loss: 60.5766970944  lr: 0.2
loss: 61.6149591604  lr: 0.2
lastloss: 58.4315473496 lr: 0.2 update: 7002

Correct: 16000 out of 16000 (1.0)

