Training set: trainingpairs-br-nonsterdutch8k
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: False
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: NONstereotypical 8k only dutch

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 132.842937469  lr: 0.2
loss: 114.493454851  lr: 0.2
loss: 110.602828419  lr: 0.2
loss: 107.053990882  lr: 0.2
loss: 98.017023107  lr: 0.2
loss: 88.4584466629  lr: 0.2
loss: 88.3551117098  lr: 0.2
loss: 84.3821673249  lr: 0.2
loss: 85.5086312769  lr: 0.2
loss: 83.5585246836  lr: 0.2
loss: 82.3719245688  lr: 0.2
loss: 83.6591187159  lr: 0.2
loss: 82.4148609823  lr: 0.2
loss: 82.7848960929  lr: 0.2
loss: 84.0357653425  lr: 0.2
loss: 84.9829212644  lr: 0.2
loss: 82.5472451661  lr: 0.2
loss: 82.4647736819  lr: 0.2
loss: 82.0485356104  lr: 0.2
loss: 82.9615490037  lr: 0.2
loss: 83.943980357  lr: 0.2
loss: 81.4164779891  lr: 0.2
loss: 82.7341080179  lr: 0.2
loss: 84.3056022644  lr: 0.2
loss: 82.6754501107  lr: 0.2
loss: 84.3332696231  lr: 0.2
loss: 81.5995062115  lr: 0.2
loss: 81.9207431309  lr: 0.2
loss: 79.4942323361  lr: 0.2
loss: 83.1311144045  lr: 0.2
loss: 82.671512495  lr: 0.2
loss: 81.7780910923  lr: 0.2
loss: 82.6022938478  lr: 0.2
loss: 82.0489134147  lr: 0.2
loss: 80.3385579155  lr: 0.2
loss: 82.5429713129  lr: 0.2
loss: 82.3150701143  lr: 0.2
loss: 81.4892134096  lr: 0.2
loss: 81.3593102961  lr: 0.2
loss: 82.109830914  lr: 0.2
loss: 83.0297076864  lr: 0.2
loss: 83.7005584943  lr: 0.2
loss: 82.2576333  lr: 0.2
loss: 81.9581704096  lr: 0.2
loss: 82.1949980137  lr: 0.2
loss: 82.0664908645  lr: 0.2
loss: 81.3872652017  lr: 0.2
loss: 81.7899178212  lr: 0.2
loss: 80.2925038663  lr: 0.2
loss: 79.9797406764  lr: 0.2
loss: 80.5362471707  lr: 0.2
loss: 82.8157966961  lr: 0.2
loss: 81.5114314406  lr: 0.2
loss: 82.297505521  lr: 0.2
loss: 82.5474515229  lr: 0.2
loss: 81.6043245014  lr: 0.2
loss: 82.3759035635  lr: 0.2
loss: 81.6867579896  lr: 0.2
loss: 82.1142800861  lr: 0.2
loss: 80.6329809481  lr: 0.2
loss: 81.1451912309  lr: 0.2
loss: 82.5271319695  lr: 0.2
loss: 82.1299916637  lr: 0.2
loss: 81.8093725433  lr: 0.2
loss: 80.9641096769  lr: 0.2
loss: 82.3255874784  lr: 0.2
loss: 81.946165168  lr: 0.2
loss: 82.001857359  lr: 0.2
loss: 80.8873735975  lr: 0.2
loss: 82.7011482394  lr: 0.2

Correct: 8000 out of 8000 (1.0)

TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 143.427282602  lr: 0.2
loss: 139.47308065  lr: 0.2
loss: 142.394306898  lr: 0.2
loss: 142.809393108  lr: 0.2
loss: 143.537183985  lr: 0.2
loss: 142.444729775  lr: 0.2
loss: 142.127463832  lr: 0.2
loss: 141.350400433  lr: 0.2
loss: 136.014057539  lr: 0.2
loss: 120.482172184  lr: 0.2
loss: 95.2542953184  lr: 0.2
loss: 84.1075654956  lr: 0.2
loss: 81.5876205984  lr: 0.2
loss: 83.3220148622  lr: 0.2
loss: 81.1177518506  lr: 0.2
loss: 81.4570142864  lr: 0.2
loss: 83.0381881946  lr: 0.2
loss: 80.802064788  lr: 0.2
loss: 80.6118171038  lr: 0.2
loss: 82.9066787763  lr: 0.2
loss: 82.4197661835  lr: 0.2
loss: 80.2150807759  lr: 0.2
loss: 79.2486274724  lr: 0.2
loss: 82.9278713514  lr: 0.2
loss: 81.4672837806  lr: 0.2
loss: 81.1211726043  lr: 0.2
loss: 83.0834812598  lr: 0.2
loss: 81.4515658205  lr: 0.2
loss: 80.3100016925  lr: 0.2
loss: 82.9787938232  lr: 0.2
loss: 81.4704728126  lr: 0.2
loss: 81.0222099515  lr: 0.2
loss: 82.4192801367  lr: 0.2
loss: 81.4225750697  lr: 0.2
loss: 80.6389893622  lr: 0.2
loss: 82.7961865101  lr: 0.2
loss: 80.9432972915  lr: 0.2
loss: 81.8171328308  lr: 0.2
loss: 80.455556636  lr: 0.2
loss: 80.5310878446  lr: 0.2
loss: 80.7765999112  lr: 0.2
loss: 81.170605836  lr: 0.2
loss: 82.6404011082  lr: 0.2
loss: 80.9933585826  lr: 0.2
loss: 82.6950820583  lr: 0.2
loss: 81.9957178303  lr: 0.2
loss: 82.1915524466  lr: 0.2
loss: 82.5352019531  lr: 0.2
loss: 81.0054545664  lr: 0.2
loss: 83.1950317097  lr: 0.2
loss: 82.1296035892  lr: 0.2
loss: 80.7804868321  lr: 0.2
loss: 80.3298630554  lr: 0.2
loss: 83.3772988615  lr: 0.2
loss: 81.5660966158  lr: 0.2
loss: 82.4065013996  lr: 0.2
loss: 81.2728249647  lr: 0.2
loss: 81.2659938481  lr: 0.2
loss: 82.0924524055  lr: 0.2
loss: 81.8381073112  lr: 0.2
loss: 82.6967541409  lr: 0.2
loss: 79.7402090416  lr: 0.2
loss: 83.1030214828  lr: 0.2
loss: 80.219520704  lr: 0.2
loss: 80.0000163431  lr: 0.2
loss: 82.8496089115  lr: 0.2
loss: 81.8804214886  lr: 0.2
loss: 81.6967669298  lr: 0.2
loss: 81.1786980592  lr: 0.2
loss: 81.084960612  lr: 0.2

Correct: 8000 out of 8000 (1.0)

