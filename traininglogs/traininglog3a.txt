Training set: trainingpairs-br-origfulldutch
Training code: p2
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: False
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: train full model without integration-only training phase


TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 141.639539778  lr: 0.2
loss: 138.664798394  lr: 0.2
loss: 140.576920912  lr: 0.2
loss: 140.382235169  lr: 0.2
loss: 135.343213543  lr: 0.2
loss: 136.536950588  lr: 0.2
loss: 128.513612747  lr: 0.2
loss: 119.132340568  lr: 0.2
loss: 123.367854247  lr: 0.2
loss: 117.360835891  lr: 0.2
loss: 105.781910872  lr: 0.2
loss: 105.611334161  lr: 0.2
loss: 103.724490288  lr: 0.2
loss: 112.712288645  lr: 0.2
loss: 97.5602963062  lr: 0.2
loss: 107.213328112  lr: 0.2
loss: 85.0644627934  lr: 0.2
loss: 99.6728683077  lr: 0.2
loss: 85.5168190368  lr: 0.2
loss: 88.1071220063  lr: 0.2
loss: 76.7853156263  lr: 0.2
loss: 80.8287942607  lr: 0.2
loss: 77.1962870297  lr: 0.2
loss: 78.4475723591  lr: 0.2
loss: 74.6268682986  lr: 0.2
loss: 81.0528915955  lr: 0.2
loss: 74.1829990043  lr: 0.2
loss: 83.4783866282  lr: 0.2
loss: 77.1396022837  lr: 0.2
loss: 72.5454116926  lr: 0.2
loss: 73.7045624937  lr: 0.2
loss: 72.1672151926  lr: 0.2
loss: 62.9180559357  lr: 0.2
loss: 70.5965382568  lr: 0.2
loss: 66.0475388569  lr: 0.2
loss: 73.8543579581  lr: 0.2
loss: 65.093731402  lr: 0.2
loss: 67.8882290876  lr: 0.2
loss: 67.261972092  lr: 0.2
loss: 64.8169823764  lr: 0.2
loss: 63.2438629799  lr: 0.2
loss: 70.6908241972  lr: 0.2
loss: 68.7419548494  lr: 0.2
loss: 65.5576282269  lr: 0.2
loss: 73.8523467358  lr: 0.2
loss: 61.013725511  lr: 0.2
loss: 61.4892356812  lr: 0.2
loss: 60.2413051263  lr: 0.2
loss: 61.3467821576  lr: 0.2
loss: 62.2645933197  lr: 0.2
loss: 61.5953058533  lr: 0.2
loss: 59.3629517373  lr: 0.2
loss: 63.7705811983  lr: 0.2
loss: 65.2385002512  lr: 0.2
loss: 66.8940756741  lr: 0.2
loss: 61.0625951472  lr: 0.2
loss: 65.8982919958  lr: 0.2
loss: 67.1947669713  lr: 0.2
loss: 60.8460273376  lr: 0.2
loss: 63.1688149755  lr: 0.2
loss: 63.3611054785  lr: 0.2
loss: 62.8753642464  lr: 0.2
loss: 61.0793134579  lr: 0.2
loss: 60.9020697059  lr: 0.2
loss: 61.3721243219  lr: 0.2
loss: 59.3016208188  lr: 0.2
loss: 60.0626615898  lr: 0.2
loss: 62.0834446617  lr: 0.2
loss: 59.033336108  lr: 0.2
loss: 67.7457790143  lr: 0.2

Correct: 16000 out of 16000 (1.0)

