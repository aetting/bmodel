Training set: trainingpairs-br-orig-ster0.25-dutch
Training code: full
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: True
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: try .25 of ster data -- with reducing lr

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 128.647069082  lr: 0.2
loss: 111.216226637  lr: 0.2
loss: 107.800893722  lr: 0.2
loss: 97.9647575403  lr: 0.2
loss: 93.3914418244  lr: 0.2
loss: 92.0218583765  lr: 0.2
loss: 84.538796474  lr: 0.2
loss: 84.8251969605  lr: 0.19
loss: 78.4861334416  lr: 0.19
loss: 79.2371723691  lr: 0.19
loss: 78.4715366674  lr: 0.19
loss: 83.0484762502  lr: 0.19
loss: 79.3759188578  lr: 0.19
loss: 80.0106669419  lr: 0.19
loss: 78.3394066925  lr: 0.1805
loss: 77.5618265816  lr: 0.1805
loss: 75.3246226229  lr: 0.1805
loss: 79.1751300587  lr: 0.1805
loss: 77.6964427257  lr: 0.1805
loss: 79.8027885535  lr: 0.1805
loss: 77.9733945914  lr: 0.1805
loss: 80.6750947815  lr: 0.171475
loss: 79.5899060736  lr: 0.171475
loss: 80.1699577888  lr: 0.171475
loss: 78.1209945195  lr: 0.171475
loss: 79.506851572  lr: 0.171475
loss: 79.6791594235  lr: 0.171475
loss: 76.682384499  lr: 0.171475
loss: 77.4789477198  lr: 0.16290125
loss: 79.0683015745  lr: 0.16290125
loss: 82.2165587681  lr: 0.16290125
loss: 77.5873775194  lr: 0.16290125
loss: 79.5237613049  lr: 0.16290125
loss: 79.8066031386  lr: 0.16290125
loss: 76.803432526  lr: 0.16290125
loss: 78.8264212246  lr: 0.1547561875
loss: 76.5447171495  lr: 0.1547561875
loss: 79.164760735  lr: 0.1547561875
loss: 74.0036641179  lr: 0.1547561875
loss: 75.9502989472  lr: 0.1547561875
loss: 77.8217072717  lr: 0.1547561875
loss: 77.6202364778  lr: 0.1547561875
loss: 79.5981910531  lr: 0.147018378125
loss: 77.0948625033  lr: 0.147018378125
loss: 74.085112456  lr: 0.147018378125
loss: 77.9019421207  lr: 0.147018378125
loss: 78.4724116777  lr: 0.147018378125
loss: 79.8757683833  lr: 0.147018378125
loss: 76.8361975368  lr: 0.147018378125
loss: 77.2252576579  lr: 0.139667459219
loss: 76.4100058856  lr: 0.139667459219
loss: 78.8483406672  lr: 0.139667459219
loss: 74.6093062189  lr: 0.139667459219
loss: 75.5810040808  lr: 0.139667459219
loss: 78.7956247177  lr: 0.139667459219
loss: 80.3182638998  lr: 0.139667459219
loss: 80.3903935588  lr: 0.132684086258
loss: 75.2954691877  lr: 0.132684086258
loss: 77.4577577469  lr: 0.132684086258
loss: 79.2908870765  lr: 0.132684086258
loss: 75.0950869033  lr: 0.132684086258
loss: 76.9768435763  lr: 0.132684086258
loss: 77.222255114  lr: 0.132684086258
loss: 80.0670849349  lr: 0.126049881945
loss: 76.102170582  lr: 0.126049881945
loss: 78.088936714  lr: 0.126049881945
loss: 78.9909627378  lr: 0.126049881945
loss: 77.4308158123  lr: 0.126049881945
loss: 74.114399793  lr: 0.126049881945
loss: 79.2723622122  lr: 0.126049881945
lastloss: 79.2723622122 lr: 0.119747387848 update: 7000

Correct: 10000 out of 10000 (1.0)



TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 155.5951498  lr: 0.2
loss: 149.970389426  lr: 0.2
loss: 150.228938922  lr: 0.2
loss: 150.555467293  lr: 0.2
loss: 147.722280756  lr: 0.2
loss: 148.143556505  lr: 0.2
loss: 143.771254547  lr: 0.2
loss: 141.926382724  lr: 0.19
loss: 133.875196995  lr: 0.19
loss: 119.248995334  lr: 0.19
loss: 97.028268344  lr: 0.19
loss: 89.2559839184  lr: 0.19
loss: 80.4751697415  lr: 0.19
loss: 79.359636916  lr: 0.19
loss: 80.5008100305  lr: 0.1805
loss: 78.543899985  lr: 0.1805
loss: 78.4871529932  lr: 0.1805
loss: 77.151483243  lr: 0.1805
loss: 77.6087890647  lr: 0.1805
loss: 74.4334610403  lr: 0.1805
loss: 77.7678649832  lr: 0.1805
loss: 78.9008662972  lr: 0.171475
loss: 74.6725765913  lr: 0.171475
loss: 77.1233578968  lr: 0.171475
loss: 75.2559751132  lr: 0.171475
loss: 80.1715168593  lr: 0.171475
loss: 76.7580228267  lr: 0.171475
loss: 77.1853965344  lr: 0.171475
loss: 75.7333277667  lr: 0.16290125
loss: 80.1305487096  lr: 0.16290125
loss: 76.3866015664  lr: 0.16290125
loss: 78.7635087303  lr: 0.16290125
loss: 80.5161809336  lr: 0.16290125
loss: 78.2536929663  lr: 0.16290125
loss: 75.372604808  lr: 0.16290125
loss: 75.0397157983  lr: 0.1547561875
loss: 76.4099065721  lr: 0.1547561875
loss: 76.0348301718  lr: 0.1547561875
loss: 77.8556749464  lr: 0.1547561875
loss: 78.2825552976  lr: 0.1547561875
loss: 79.0115637501  lr: 0.1547561875
loss: 76.7987862354  lr: 0.1547561875
loss: 77.6747497862  lr: 0.147018378125
loss: 78.0723734724  lr: 0.147018378125
loss: 74.0290058164  lr: 0.147018378125
loss: 78.6830891599  lr: 0.147018378125
loss: 76.4591834683  lr: 0.147018378125
loss: 76.115604827  lr: 0.147018378125
loss: 77.9201369714  lr: 0.147018378125
loss: 79.3767610157  lr: 0.139667459219
loss: 79.1727583807  lr: 0.139667459219
loss: 76.7550312454  lr: 0.139667459219
loss: 79.8363958989  lr: 0.139667459219
loss: 79.4567030964  lr: 0.139667459219
loss: 76.7704786676  lr: 0.139667459219
loss: 78.8954600755  lr: 0.139667459219
loss: 76.6828650877  lr: 0.132684086258
loss: 75.2209783251  lr: 0.132684086258
loss: 79.1205699365  lr: 0.132684086258
loss: 77.0289681886  lr: 0.132684086258
loss: 78.9948048628  lr: 0.132684086258
loss: 78.1841050963  lr: 0.132684086258
loss: 74.9319463693  lr: 0.132684086258
loss: 77.6455520248  lr: 0.126049881945
loss: 77.2915013271  lr: 0.126049881945
loss: 76.3078520141  lr: 0.126049881945
loss: 76.4982487272  lr: 0.126049881945
loss: 79.122800384  lr: 0.126049881945
loss: 76.4629462014  lr: 0.126049881945
loss: 76.5879318461  lr: 0.126049881945