Training set: trainingpairs-br-orig-ster0.5-dutch
Training code: full
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: True
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: try .5 of ster data again -- this time with reducing lr

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 122.432958595  lr: 0.2
loss: 107.125824362  lr: 0.2
loss: 92.4039138164  lr: 0.2
loss: 100.156441653  lr: 0.2
loss: 93.2998476992  lr: 0.2
loss: 91.9543012681  lr: 0.2
loss: 84.2078579178  lr: 0.2
loss: 80.652649162  lr: 0.19
loss: 76.9894084529  lr: 0.19
loss: 73.3010280585  lr: 0.19
loss: 72.9283647945  lr: 0.19
loss: 69.7754568056  lr: 0.19
loss: 72.631747323  lr: 0.19
loss: 72.2471065016  lr: 0.19
loss: 68.0805169345  lr: 0.1805
loss: 72.5627275166  lr: 0.1805
loss: 70.9117354311  lr: 0.1805
loss: 69.0533621215  lr: 0.1805
loss: 72.3854966703  lr: 0.1805
loss: 70.1225777446  lr: 0.1805
loss: 68.3699153666  lr: 0.1805
loss: 77.2638780479  lr: 0.171475
loss: 75.1057437696  lr: 0.171475
loss: 67.4533032367  lr: 0.171475
loss: 67.7340942539  lr: 0.171475
loss: 74.2140022691  lr: 0.171475
loss: 71.9394739305  lr: 0.171475
loss: 72.4284399746  lr: 0.171475
loss: 69.0123053719  lr: 0.16290125
loss: 71.5808785054  lr: 0.16290125
loss: 73.0825376647  lr: 0.16290125
loss: 74.927742189  lr: 0.16290125
loss: 72.3318922216  lr: 0.16290125
loss: 72.6487327928  lr: 0.16290125
loss: 70.1590010732  lr: 0.16290125
loss: 70.2666026785  lr: 0.1547561875
loss: 73.2847416796  lr: 0.1547561875
loss: 72.5947452382  lr: 0.1547561875
loss: 69.310386431  lr: 0.1547561875
loss: 68.8631250007  lr: 0.1547561875
loss: 73.9504220191  lr: 0.1547561875
loss: 74.9711579396  lr: 0.1547561875
loss: 71.8474549262  lr: 0.147018378125
loss: 72.0467044374  lr: 0.147018378125
loss: 65.1759614495  lr: 0.147018378125
loss: 70.5884601665  lr: 0.147018378125
loss: 72.7750207968  lr: 0.147018378125
loss: 72.6460524256  lr: 0.147018378125
loss: 68.9347198917  lr: 0.147018378125
loss: 68.5525528821  lr: 0.139667459219
loss: 74.5116912886  lr: 0.139667459219
loss: 69.4474057177  lr: 0.139667459219
loss: 76.9330756452  lr: 0.139667459219
loss: 70.4260400402  lr: 0.139667459219
loss: 68.5522898963  lr: 0.139667459219
loss: 67.9326004988  lr: 0.139667459219
loss: 75.3420776482  lr: 0.132684086258
loss: 71.3001963427  lr: 0.132684086258
loss: 72.7020041439  lr: 0.132684086258
loss: 75.9328902378  lr: 0.132684086258
loss: 73.7578522864  lr: 0.132684086258
loss: 75.8713773254  lr: 0.132684086258
loss: 69.0914883183  lr: 0.132684086258
loss: 71.666965378  lr: 0.126049881945
loss: 71.5731706166  lr: 0.126049881945
loss: 70.9508759687  lr: 0.126049881945
loss: 69.1338004598  lr: 0.126049881945
loss: 71.4812252509  lr: 0.126049881945
loss: 78.056757754  lr: 0.126049881945
loss: 73.498070564  lr: 0.126049881945
lastloss: 70.7055608688 lr: 0.119747387848 update: 7001

Correct: 12000 out of 12000 (1.0)



TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 154.171383828  lr: 0.2
loss: 148.478051007  lr: 0.2
loss: 127.914131598  lr: 0.2
loss: 112.430246987  lr: 0.2
loss: 86.2657256229  lr: 0.2
loss: 71.911929467  lr: 0.2
loss: 73.9114042404  lr: 0.2
loss: 71.4744091631  lr: 0.19
loss: 73.3365670118  lr: 0.19
loss: 71.9382234261  lr: 0.19
loss: 68.9419746501  lr: 0.19
loss: 72.5241983311  lr: 0.19
loss: 67.3616189169  lr: 0.19
loss: 69.6439181928  lr: 0.19
loss: 67.6863919058  lr: 0.1805
loss: 73.92056204  lr: 0.1805
loss: 68.1725159307  lr: 0.1805
loss: 75.079639105  lr: 0.1805
loss: 73.4852958456  lr: 0.1805
loss: 71.4257632903  lr: 0.1805
loss: 69.0326876117  lr: 0.1805
loss: 72.3614186335  lr: 0.171475
loss: 70.0714669332  lr: 0.171475
loss: 71.9500993495  lr: 0.171475
loss: 78.2055814253  lr: 0.171475
loss: 75.2687266256  lr: 0.171475
loss: 73.5851404474  lr: 0.171475
loss: 68.2629046137  lr: 0.171475
loss: 76.8917612865  lr: 0.16290125
loss: 71.5943625929  lr: 0.16290125
loss: 71.7945211726  lr: 0.16290125
loss: 69.9398628942  lr: 0.16290125
loss: 72.1393357002  lr: 0.16290125
loss: 65.0548002677  lr: 0.16290125
loss: 73.9635706553  lr: 0.16290125
loss: 71.3209803346  lr: 0.1547561875
loss: 73.7095916604  lr: 0.1547561875
loss: 71.3701310145  lr: 0.1547561875
loss: 66.2766107673  lr: 0.1547561875
loss: 67.3122310398  lr: 0.1547561875
loss: 73.8970817697  lr: 0.1547561875
loss: 66.8438442412  lr: 0.1547561875
loss: 69.3899740657  lr: 0.147018378125
loss: 68.3761636456  lr: 0.147018378125
loss: 71.1497905453  lr: 0.147018378125
loss: 72.9175185864  lr: 0.147018378125
loss: 67.9196555116  lr: 0.147018378125
loss: 75.5288705221  lr: 0.147018378125
loss: 72.2155823101  lr: 0.147018378125
loss: 76.9718479746  lr: 0.139667459219
loss: 71.1133169653  lr: 0.139667459219
loss: 74.0094674768  lr: 0.139667459219
loss: 70.5527960744  lr: 0.139667459219
loss: 71.6267586944  lr: 0.139667459219
loss: 66.6193007258  lr: 0.139667459219
loss: 69.8183762026  lr: 0.139667459219
loss: 70.6834865624  lr: 0.132684086258
loss: 65.3025489121  lr: 0.132684086258
loss: 73.4611773146  lr: 0.132684086258
loss: 76.9735165872  lr: 0.132684086258
loss: 71.3530665941  lr: 0.132684086258
loss: 66.905625315  lr: 0.132684086258
loss: 74.4015264567  lr: 0.132684086258
loss: 76.133197514  lr: 0.126049881945
loss: 67.2314020649  lr: 0.126049881945
loss: 67.5297347021  lr: 0.126049881945
loss: 68.6353088815  lr: 0.126049881945
loss: 74.9648088277  lr: 0.126049881945
loss: 71.9754479201  lr: 0.126049881945
loss: 74.2993843896  lr: 0.126049881945
lastloss: 70.6981355917 lr: 0.119747387848 update: 7001

Correct: 12000 out of 12000 (1.0)

