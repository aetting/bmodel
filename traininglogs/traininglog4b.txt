Training set: trainingpairs-br-holdoutB-dutch
Training code: full
Embeddings: embs/brouwerCOALS-100.txt
Binary: True
Reduce lr: False
Context 200, Retrieval 80
Vocab 35, Embedding 100

Notes: train on holdout data: stereotypical sentences are active only (as before, but now make up full half of data), and all-combinations has no sentences, active or passive, from the simulation triplets

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 111.087400295  lr: 0.2
loss: 74.8865334676  lr: 0.2
loss: 78.3495389326  lr: 0.2
loss: 74.1961138812  lr: 0.2
loss: 70.3660754183  lr: 0.2
loss: 63.681739979  lr: 0.2
loss: 70.1760228008  lr: 0.2
loss: 61.2582945855  lr: 0.2
loss: 64.2763720464  lr: 0.2
loss: 57.523819268  lr: 0.2
loss: 64.2380912977  lr: 0.2
loss: 59.0755322191  lr: 0.2
loss: 60.3824856899  lr: 0.2
loss: 56.7950330743  lr: 0.2
loss: 65.0550197118  lr: 0.2
loss: 62.5593737541  lr: 0.2
loss: 58.746623853  lr: 0.2
loss: 63.5914108564  lr: 0.2
loss: 59.0396388604  lr: 0.2
loss: 61.6342030113  lr: 0.2
loss: 55.6498747666  lr: 0.2
loss: 56.9315452355  lr: 0.2
loss: 60.318953153  lr: 0.2
loss: 53.6551662912  lr: 0.2
loss: 59.3901698411  lr: 0.2
loss: 60.2974533904  lr: 0.2
loss: 56.6019044939  lr: 0.2
loss: 58.3164991341  lr: 0.2
loss: 60.7135439158  lr: 0.2
loss: 53.990472165  lr: 0.2
loss: 61.1897196403  lr: 0.2
loss: 53.5807716557  lr: 0.2
loss: 54.9407918535  lr: 0.2
loss: 58.2945429388  lr: 0.2
loss: 61.9916881431  lr: 0.2
loss: 55.7943554297  lr: 0.2
loss: 55.8975981767  lr: 0.2
loss: 53.8044255755  lr: 0.2
loss: 47.5198404989  lr: 0.2
loss: 63.9638056502  lr: 0.2
loss: 53.9885170449  lr: 0.2
loss: 57.4857758122  lr: 0.2
loss: 56.5850596352  lr: 0.2
loss: 56.3669656025  lr: 0.2
loss: 54.5724077442  lr: 0.2
loss: 60.7718914335  lr: 0.2
loss: 56.5513850234  lr: 0.2
loss: 64.3876697019  lr: 0.2
loss: 52.7229216985  lr: 0.2
loss: 60.6807828292  lr: 0.2
loss: 58.3619987105  lr: 0.2
loss: 55.3473314808  lr: 0.2
loss: 57.8554584661  lr: 0.2
loss: 53.4823888672  lr: 0.2
loss: 56.8068516168  lr: 0.2
loss: 55.1504559916  lr: 0.2
loss: 53.0654935302  lr: 0.2
loss: 59.3178741376  lr: 0.2
loss: 53.5954669634  lr: 0.2
loss: 59.161479019  lr: 0.2
loss: 52.3529647099  lr: 0.2
loss: 58.305389865  lr: 0.2
loss: 57.6009216508  lr: 0.2
loss: 55.0820337375  lr: 0.2
loss: 57.8934711026  lr: 0.2
loss: 57.9137373782  lr: 0.2
loss: 53.5543056873  lr: 0.2
loss: 55.5380505644  lr: 0.2
loss: 56.8454429887  lr: 0.2
loss: 50.5486006045  lr: 0.2
lastloss: 50.5486006045 lr: 0.2 update: 7000

Correct: 15840 out of 15840 (1.0)



TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 153.736087069  lr: 0.2
loss: 147.328209169  lr: 0.2
loss: 133.043778296  lr: 0.2
loss: 98.4813286825  lr: 0.2
loss: 80.0801991259  lr: 0.2
loss: 74.2237521014  lr: 0.2
loss: 56.9188074977  lr: 0.2
loss: 58.4764770774  lr: 0.2
loss: 62.8225780725  lr: 0.2
loss: 58.0141106114  lr: 0.2
loss: 52.6307087232  lr: 0.2
loss: 54.0346118046  lr: 0.2
loss: 59.9508914885  lr: 0.2
loss: 52.8712251159  lr: 0.2
loss: 58.5235995658  lr: 0.2
loss: 55.1828902011  lr: 0.2
loss: 58.0549174052  lr: 0.2
loss: 56.4405528998  lr: 0.2
loss: 59.6302865715  lr: 0.2
loss: 55.1150324625  lr: 0.2
loss: 52.5148836292  lr: 0.2
loss: 58.6878517858  lr: 0.2
loss: 61.5629085506  lr: 0.2
loss: 64.5793293675  lr: 0.2
loss: 60.5400450946  lr: 0.2
loss: 52.844813028  lr: 0.2
loss: 55.7303398173  lr: 0.2
loss: 54.9756594022  lr: 0.2
loss: 56.9539073228  lr: 0.2
loss: 51.028421917  lr: 0.2
loss: 52.7812031765  lr: 0.2
loss: 58.9765958377  lr: 0.2
loss: 56.0693651848  lr: 0.2
loss: 56.1106677779  lr: 0.2
loss: 51.3217389484  lr: 0.2
loss: 53.4109069399  lr: 0.2
loss: 55.8975205448  lr: 0.2
loss: 53.7088274631  lr: 0.2
loss: 52.7641407456  lr: 0.2
loss: 59.0594973704  lr: 0.2
loss: 50.6692740695  lr: 0.2
loss: 55.4543842681  lr: 0.2
loss: 63.6142934079  lr: 0.2
loss: 56.3563485699  lr: 0.2
loss: 54.8202049673  lr: 0.2
loss: 59.2470200356  lr: 0.2
loss: 58.9967454628  lr: 0.2
loss: 50.089667295  lr: 0.2
loss: 65.1805413422  lr: 0.2
loss: 54.4375349974  lr: 0.2
loss: 64.2958353121  lr: 0.2
loss: 53.4740394363  lr: 0.2
loss: 63.5666044136  lr: 0.2
loss: 55.7846920867  lr: 0.2
loss: 54.4083159627  lr: 0.2
loss: 50.1312756137  lr: 0.2
loss: 56.3013528249  lr: 0.2
loss: 51.3977493931  lr: 0.2
loss: 51.9962863801  lr: 0.2
loss: 55.9527819907  lr: 0.2
loss: 55.4555815551  lr: 0.2
loss: 59.19947113  lr: 0.2
loss: 61.4328954388  lr: 0.2
loss: 58.1896056313  lr: 0.2
loss: 53.1608442672  lr: 0.2
loss: 50.1486369602  lr: 0.2
loss: 63.2220328971  lr: 0.2
loss: 57.9867768715  lr: 0.2
loss: 58.2708129867  lr: 0.2
loss: 60.8914365291  lr: 0.2
lastloss: 59.4666017329 lr: 0.2 update: 7001

Correct: 15840 out of 15840 (1.0)

