Training set: trainingpairs-br-origfulldutch
Embeddings: brouwerCOALS-100.txt
Binary: True
Context 200, Retrieval 80
Vocab 35, Embedding 100

TRAINING PART ONE

100 items per update, 7000 total updates

NetInteg (
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 112.600797631  lr: 0.2
loss: 89.2899507796  lr: 0.2
loss: 93.8210450932  lr: 0.2
loss: 89.6967246863  lr: 0.2
loss: 87.4886773595  lr: 0.2
loss: 75.1438160032  lr: 0.2
loss: 74.536320965  lr: 0.2
loss: 69.0792563651  lr: 0.19
loss: 79.9634866529  lr: 0.19
loss: 67.876173708  lr: 0.19
loss: 69.9374407409  lr: 0.19
loss: 68.966190265  lr: 0.19
loss: 61.3467294581  lr: 0.19
loss: 63.6954152007  lr: 0.19
loss: 65.6129150604  lr: 0.1805
loss: 65.9316322255  lr: 0.1805
loss: 64.1337370704  lr: 0.1805
loss: 70.32808944  lr: 0.1805
loss: 62.6499732534  lr: 0.1805
loss: 68.1420746893  lr: 0.1805
loss: 65.5729995592  lr: 0.1805
loss: 66.6103524525  lr: 0.171475
loss: 61.9950264286  lr: 0.171475
loss: 59.239134004  lr: 0.171475
loss: 55.9371675796  lr: 0.171475
loss: 61.3745689687  lr: 0.171475
loss: 58.9806258836  lr: 0.171475
loss: 66.2090773861  lr: 0.171475
loss: 62.6646431822  lr: 0.16290125
loss: 56.5260191788  lr: 0.16290125
loss: 60.6398160878  lr: 0.16290125
loss: 61.6563999127  lr: 0.16290125
loss: 56.4664774501  lr: 0.16290125
loss: 65.4070902296  lr: 0.16290125
loss: 68.4561600835  lr: 0.16290125
loss: 71.8863250197  lr: 0.1547561875
loss: 62.6321304346  lr: 0.1547561875
loss: 60.9708958442  lr: 0.1547561875
loss: 58.3012908537  lr: 0.1547561875
loss: 62.1121430333  lr: 0.1547561875
loss: 61.0744117502  lr: 0.1547561875
loss: 60.4597898506  lr: 0.1547561875
loss: 58.2736880363  lr: 0.147018378125
loss: 65.6697283719  lr: 0.147018378125
loss: 65.7377755702  lr: 0.147018378125
loss: 59.174563047  lr: 0.147018378125
loss: 60.874683707  lr: 0.147018378125
loss: 61.6635578744  lr: 0.147018378125
loss: 58.7730278178  lr: 0.147018378125
loss: 62.9760575944  lr: 0.139667459219
loss: 63.8780415164  lr: 0.139667459219
loss: 63.9567810159  lr: 0.139667459219
loss: 56.8870310351  lr: 0.139667459219
loss: 61.3283338596  lr: 0.139667459219
loss: 61.3446751503  lr: 0.139667459219
loss: 62.8117501899  lr: 0.139667459219
loss: 59.5909777339  lr: 0.132684086258
loss: 55.8003375159  lr: 0.132684086258
loss: 57.3482985715  lr: 0.132684086258
loss: 53.6347688654  lr: 0.132684086258
loss: 55.8561911725  lr: 0.132684086258
loss: 61.9969208245  lr: 0.132684086258
loss: 59.512441238  lr: 0.132684086258
loss: 55.8587487977  lr: 0.126049881945
loss: 56.8036767511  lr: 0.126049881945
loss: 66.4329789395  lr: 0.126049881945
loss: 66.6282221302  lr: 0.126049881945
loss: 62.9919457424  lr: 0.126049881945
loss: 60.5071555013  lr: 0.126049881945
loss: 60.3623347222  lr: 0.126049881945

Correct: 16000 out of 16000 (1.0)

TRAINING PART TWO

100 items per update, 7000 total updates

NetFull (
  (retr): Linear (235 -> 80)
  (retr_out): Linear (80 -> 100)
  (integ): Linear (300 -> 200)
  (integ_out): Linear (200 -> 300)
)

loss: 151.345938623  lr: 0.2
loss: 136.07999137  lr: 0.2
loss: 98.1925351506  lr: 0.2
loss: 90.8820289349  lr: 0.2
loss: 79.1643240084  lr: 0.2
loss: 66.254313089  lr: 0.2
loss: 69.7762104512  lr: 0.2
loss: 70.0228065806  lr: 0.19
loss: 66.5930010872  lr: 0.19
loss: 66.853952508  lr: 0.19
loss: 57.317509893  lr: 0.19
loss: 51.0571060985  lr: 0.19
loss: 56.2626533893  lr: 0.19
loss: 59.7617092038  lr: 0.19
loss: 59.611470187  lr: 0.1805
loss: 65.5047678276  lr: 0.1805
loss: 57.630664147  lr: 0.1805
loss: 60.1304665431  lr: 0.1805
loss: 64.015578301  lr: 0.1805
loss: 56.8390156845  lr: 0.1805
loss: 65.1293753607  lr: 0.1805
loss: 60.7220635736  lr: 0.171475
loss: 70.7971616595  lr: 0.171475
loss: 62.5146580063  lr: 0.171475
loss: 59.3635050961  lr: 0.171475
loss: 56.429644985  lr: 0.171475
loss: 58.2419621937  lr: 0.171475
loss: 59.2813687711  lr: 0.171475
loss: 59.6432909677  lr: 0.16290125
loss: 60.8481695985  lr: 0.16290125
loss: 59.3943831653  lr: 0.16290125
loss: 63.0961356393  lr: 0.16290125
loss: 67.3208321654  lr: 0.16290125
loss: 63.267713687  lr: 0.16290125
loss: 56.2692698327  lr: 0.16290125
loss: 60.983211318  lr: 0.1547561875
loss: 64.5872897494  lr: 0.1547561875
loss: 59.6632612527  lr: 0.1547561875
loss: 68.4021554169  lr: 0.1547561875
loss: 66.1988410277  lr: 0.1547561875
loss: 58.0557728072  lr: 0.1547561875
loss: 68.0774417453  lr: 0.1547561875
loss: 60.8508656586  lr: 0.147018378125
loss: 66.6783273737  lr: 0.147018378125
loss: 61.4523245364  lr: 0.147018378125
loss: 68.1309102149  lr: 0.147018378125
loss: 61.3288423431  lr: 0.147018378125
loss: 58.3636432295  lr: 0.147018378125
loss: 61.2054280076  lr: 0.147018378125
loss: 58.7581438357  lr: 0.139667459219
loss: 60.2730906176  lr: 0.139667459219
loss: 60.5179809046  lr: 0.139667459219
loss: 66.7450065799  lr: 0.139667459219
loss: 59.5048748383  lr: 0.139667459219
loss: 59.0354323082  lr: 0.139667459219
loss: 56.9204552015  lr: 0.139667459219
loss: 68.6562541802  lr: 0.132684086258
loss: 58.7227406113  lr: 0.132684086258
loss: 61.7549032749  lr: 0.132684086258
loss: 68.2163985221  lr: 0.132684086258
loss: 60.4308192167  lr: 0.132684086258
loss: 57.7496600843  lr: 0.132684086258
loss: 55.4577515864  lr: 0.132684086258
loss: 63.852498045  lr: 0.126049881945
loss: 63.7393934997  lr: 0.126049881945
loss: 61.862788301  lr: 0.126049881945
loss: 58.1026936489  lr: 0.126049881945
loss: 62.9676514937  lr: 0.126049881945
loss: 58.3942217698  lr: 0.126049881945
loss: 65.5102075908  lr: 0.126049881945

Correct: 16000 out of 16000 (1.0)

